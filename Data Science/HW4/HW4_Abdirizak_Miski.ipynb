{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Evaluation Metrics and Custom Feature Selection\n",
    "Some classical evaluation metrics occasionally fall short under certain circumstances. In the first part of this homework, you will implement a few special evaluation metrics used in predictive analytics that is designated for one-sided and imbalanced prediction tasks.\n",
    "\n",
    "In the second part, you will be implementing a custom feature selection procedure. This will be a univariate feature selection method. You will be given a toy dataset called 'Car Evaluation Data Set' (see: http://archive.ics.uci.edu/ml/datasets/Car+Evaluation for details). You are not required to, but advised to test your code with the toy dataset, or any other dataset that contains categorical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Forecast Metrics\n",
    "There are many types of forecasts, each calling for slightly different methods of verification. The primary target for the measures below are dichotomous forecasts (in other words, binary predictions). \n",
    "\n",
    "The forecast terminology is slightly different. Following are the differences:\n",
    "- Instead of True Positives (TP) --> _Hits_ (a) is used, \n",
    "- Instead of False Positives (FP) --> _False Alarms_ (b) is used,\n",
    "- Instead of False Negatives (FN) --> _Misses_ (c) is used, \n",
    "- Instead of True Negatives (TN) --> _Correct Rejects_ (d) is used.\n",
    "\n",
    "For every single model run, you are given:\n",
    "1. a set of observations (Event [1] or No Event [0]) \n",
    "2. a set of prediction scores (a float between 0 and 1) and an event threshold, where the predicted outcome will be \n",
    "\n",
    "<center>$\\hat{y}= \n",
    "\\begin{cases}\n",
    "    1 \\text{ (Event occurred)},& \\text{if } score \\geq threshold\\\\\n",
    "    0 \\text{ (No event),}      & \\text{if } score < threshold\n",
    "\\end{cases} $ <center>\n",
    "\n",
    "Note that observations are ground truth and prediction scores and threshold will be used for determining the predicted model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Here are some test data you can use for this homework\n",
    "Y_test = [0, 0, 1, 1, 0, 1, 0, 0, 0, 1] # observations\n",
    "P_scores = [0.1, 0.32, 0.48, 0.9, 0.6, 0.55, 0.42, 0.37, 0.61, 0.66] # prediction scores\n",
    "threshold = 0.5 # prediction threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 1 (10 points)\n",
    "Given prediction scores, threshold, and observations create a function to determine the elements of a confusion matrix. For ease of use, you will output a `dict` (dictionary) object instead of a 2-dimensional numpy array. Note that _positive_ corresponds to the event occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 3, 'FP': 1, 'TN': 4, 'FN': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_conf_matrix(observation, p_scores, threshold):\n",
    "    predictions = []\n",
    "\n",
    "    #filling in a list of prediction scores based on the threshold \n",
    "    for p in p_scores:\n",
    "        if p >= threshold:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    #iterate through list of predictions and find elements of a confusion matrix\n",
    "    for i in range(len(predictions)):\n",
    "        if(observation[i] ==1 ):\n",
    "            if(predictions[i] == 1):\n",
    "                TP +=1\n",
    "            else:\n",
    "                FP +=1\n",
    "        else:\n",
    "            if(predictions[i] == 0):\n",
    "                TN +=1\n",
    "            else:\n",
    "                FN +=1\n",
    "    \n",
    "    return {'TP':TP, 'FP':FP, 'TN':TN, 'FN':FN}\n",
    "\n",
    "\n",
    "bc_matrix = binary_conf_matrix(Y_test, P_scores, threshold)\n",
    "bc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 2 (10 points)\n",
    "\n",
    "Create functions for calculating accuracy, precision, recall, and F1-score. You can use the definitions from slides (Chapter 11). (You are supposed to calculate the precision and recall (and thus F1-score) for 'Event' [1] class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7\n",
      "precision = 0.75\n",
      "recall = 0.6\n",
      "f1score = 0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "#accuracy = (tp+tn)/(tp+tn+tp+tn)\n",
    "def accuracy(observation, p_scores, threshold):\n",
    "    bc_matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    TP, TN, FP, FN = bc_matrix['TP'], bc_matrix['TN'],bc_matrix['FP'], bc_matrix['FN']\n",
    "    return (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "#precision = tp/(tp+fp)    \n",
    "def precision(observation, p_scores, threshold):\n",
    "    bc_matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    TP,FP= bc_matrix['TP'],bc_matrix['FP']\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "#recall = tp/(tp+fn)\n",
    "def recall(observation, p_scores, threshold):\n",
    "    bc_matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    TP,FN = bc_matrix['TP'],bc_matrix['FN']\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "#f1= 2*((precision*recall)/(precision + recall))\n",
    "def f1score(observation, p_scores, threshold):\n",
    "    precis = precision(observation, p_scores, threshold )\n",
    "    rec = recall( observation, p_scores, threshold )\n",
    "    return 2*(precis*rec)/(precis+rec)\n",
    "\n",
    "    \n",
    "print(\"accuracy =\", accuracy( Y_test, P_scores, threshold ))\n",
    "\n",
    "print(\"precision =\",precision( Y_test, P_scores, threshold ))\n",
    "\n",
    "print(\"recall =\",recall( Y_test, P_scores, threshold ))\n",
    "\n",
    "print(\"f1score =\", f1score( Y_test, P_scores, threshold ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 3 (5 points)\n",
    "Calculate the bias score (BIAS). Bias score measures the ratio of the frequency of predicted event occurrences to the frequency of observed events. It can be calculated using the following formula:\n",
    "\n",
    "$ BIAS = \\frac{\\text{hits} + \\text{false alarms} }{ \\text{hits} + \\text{misses} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIAS = 0.8\n"
     ]
    }
   ],
   "source": [
    "def bias_score(observation, p_scores, threshold):\n",
    "    bc_matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    TP, TN, FP, FN = bc_matrix['TP'], bc_matrix['TN'],bc_matrix['FP'], bc_matrix['FN']\n",
    "    return (TP+FP)/(TP+FN)\n",
    "\n",
    "    \n",
    "print(\"BIAS =\", bias_score(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 4 (5 points)\n",
    "\n",
    "Calculate the false alarm ratio (FAR). FAR is an indicator of what fraction of the predicted events actually did not occur (i.e., they were false alarms)? You can calculate the FAR as follows: \n",
    "\n",
    "$ FAR = \\frac{ \\text{false alarms} }{ \\text{hits} + \\text{false alarms} }  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAR = 0.25\n"
     ]
    }
   ],
   "source": [
    "def far(observation, p_scores, threshold):\n",
    "    TP,FP = bc_matrix['TP'],bc_matrix['FP']\n",
    "    return FP/(TP+FP)\n",
    "\n",
    "print(\"FAR =\", far(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 5 (5 points)\n",
    "Calculate the threat score. Threat score (which is also referred to as critical success index -- CSI) indicates how well did the predicted event outcomes correspond to the observed events. It measures the fraction of actual __and/or__ predicted events that were correctly predicted. It can be thought of as the accuracy when the correct negatives (TN) have been removed from consideration. It can be calculated as follows:\n",
    "\n",
    "$CSI = \\frac{ \\text{hits} }{ \\text{hits} + \\text{misses} + \\text{false alarms} } $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSI = 0.5\n"
     ]
    }
   ],
   "source": [
    "def csi(observation, p_scores, threshold):\n",
    "    TP, FP, FN = bc_matrix['TP'], bc_matrix['FP'], bc_matrix['FN']\n",
    "    return TP/(TP+FN+FP)\n",
    "\n",
    "\n",
    "print(\"CSI =\",csi (Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 6 (7.5 points)\n",
    "Skill scores are often used to understand the improvement of model performance over a given baseline (often a hypothetical, predetermined random forecast result). The first skill score you will implement is _true skill statistic (TSS)_ (which is also known as Hanssen-Kuipers Skill Score. It can be used to understand how well the prediction model separate the events (detected) from the no events. TSS can be calculated as follows:\n",
    "\n",
    "$ TSS = \\frac{\\text{hits} }{\\text{hits} + \\text{misses}} - \\frac{\\text{false alarms} }{\\text{false alarms} + \\text{correct negatives}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tss = 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "def tss(observation, p_scores, threshold):\n",
    "     TP, TN, FP, FN = bc_matrix['TP'], bc_matrix['TN'],bc_matrix['FP'], bc_matrix['FN']\n",
    "     return (TP/(TP+FN)) - (FP/(FP+TN))\n",
    "\n",
    "\n",
    "print(\"tss =\", tss(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Question 7 (7.5 points)\n",
    "The next skill score to calculate is Gilbert Skill Score (also known as Equitable Threat Score). This is an interesting indicator of performance because it measures the fraction of observed and/or predicted events that were correctly predicted, adjusted for hits (correctly predicted events) associated with random chance. For example, it is easier to correctly predict rain occurrence in a wet climate than in a dry climate. In other words, GSS can answer how well the predicted event occurrences correspond to the observed events (accounting for correct predictions appearing due to chance). The Gilbert Skill Score can be calculated as follows:\n",
    "\n",
    "$GSS = \\frac{ \\text{hits} - \\text{hits}_{random}}{ \\text{hits} + \\text{misses} + \\text{false alarms} - \\text{hits}_{random} } $\n",
    "\n",
    "where the random hits can ba calculated as:\n",
    "\n",
    "$ \\text{hits}_{random} = \\frac{ (\\text{hits}+\\text{misses} )* (\\text{hits}+\\text{false alarms} )}{total} $.\n",
    "\n",
    "_total_ is sum of all the elements in confusion matrix. \n",
    "\n",
    "Hint: Notice the similarity between GSS and threat score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gss = 0.25\n"
     ]
    }
   ],
   "source": [
    "def gss(observation, p_scores, threshold):\n",
    "    TP, TN, FP, FN = bc_matrix['TP'], bc_matrix['TN'],bc_matrix['FP'], bc_matrix['FN']\n",
    "    hits_random = (TP + FN)*(TP+FP) / (TP+TN+FP+FN)\n",
    "    return (TP-hits_random)/(TP+FN+FP-hits_random)\n",
    "\n",
    "print(\"gss =\", gss(Y_test, P_scores, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Bonus Question (10 points)\n",
    "\n",
    "Your last task is to determine an optimal threshold based on prediction scores, observations, and a given performance measure. Create a function called `pick_threshold` which will pick the best prediction score threshold (that will return the highest performance measure based on the given performance metric). Hint: Python allows you to pass a (performance measure) function (such as `tss`, `csi`, or `f1score`) to `pick_threshold`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gss_threshold: 0.01 gss_score: 0.25\n",
      "tss_threshold: 0.01 tss_score: 0.39999999999999997\n",
      "csi_threshold: 0.01 csi_score: 0.5\n",
      "f1_threshold: 0.42000000000000004 f1_score: 0.8\n"
     ]
    }
   ],
   "source": [
    "def pick_threshold(observation, p_scores, mfunc):\n",
    "\n",
    "    optimal_threshold = 0 \n",
    "    best_score = 0\n",
    "\n",
    "    #iterate through range of values [.01-1]\n",
    "    #set score = mfunc() performance measure\n",
    "    #if score is higher than best score ->swap and threshold gets set to that i \n",
    "    for i in np.arange(.01,1,0.01): \n",
    "        try :\n",
    "            score =mfunc(observation, p_scores, i)\n",
    "        except:\n",
    "            score = 0\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score \n",
    "            optimal_threshold = i \n",
    "            \n",
    "    return optimal_threshold,best_score\n",
    "\n",
    "\n",
    "gss_threshold, gss_score = pick_threshold(Y_test, P_scores, gss)\n",
    "print(\"gss_threshold:\",gss_threshold, \"gss_score:\",gss_score)\n",
    "\n",
    "tss_threshold, tss_score = pick_threshold(Y_test, P_scores, tss)\n",
    "print(\"tss_threshold:\", tss_threshold, \"tss_score:\",tss_score)\n",
    "\n",
    "csi_threshold, csi_score = pick_threshold(Y_test, P_scores, csi)\n",
    "print(\"csi_threshold:\", csi_threshold, \"csi_score:\",csi_score)\n",
    "\n",
    "f1_threshold, f1_score = pick_threshold(Y_test, P_scores, f1score)\n",
    "print(\"f1_threshold:\", f1_threshold, \"f1_score:\",f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: IUFS Implementation\n",
    "\n",
    "In this part, you are advised to use the car evaluation dataset. The given dataset contains six descriptive features and a target variable. Each of those are ordinal scale, categorical variables. The name of the target feature is 'evaluation'.\n",
    "\n",
    "Note here that you are expected to write your own code for the feature selection method. Therefore, DO NOT COPY AND PASTE CODE OR USE LIBRARY FUNCTIONS. The goal of the homework is not to see if you can call library functions but to have you practice with the impurity measures and feature selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   buying      1728 non-null   object\n",
      " 1   maint       1728 non-null   object\n",
      " 2   doors       1728 non-null   object\n",
      " 3   persons     1728 non-null   object\n",
      " 4   lug_boot    1728 non-null   object\n",
      " 5   safety      1728 non-null   object\n",
      " 6   evaluation  1728 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 94.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>576</td>\n",
       "      <td>576</td>\n",
       "      <td>576</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       buying  maint doors persons lug_boot safety evaluation\n",
       "count    1728   1728  1728    1728     1728   1728       1728\n",
       "unique      4      4     4       3        3      3          4\n",
       "top     vhigh  vhigh     2       2    small    low      unacc\n",
       "freq      432    432   432     576      576    576       1210"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf = pd.read_csv('careval.csv')\n",
    "# edf.head()\n",
    "edf.info()\n",
    "edf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a feature selection method called IUFS (impurity-based univariate feature selection), which will select the most informative features with a univariate filter feature selection schema. This feature selection method will take the dataset, name of the target variable, number of features to be selected (k) and the measure of impurity as an input, and will output the names of k best features based on the information gain. You are expected to implement information gain, entropy and Gini index functions. Note here that this will be a univariate selection, which means that you need to test the features individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 1 (10 points)\n",
    "Implement a function that calculates entropy for a feature in a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy(buying) = 2.0\n",
      "entropy(evaluation) =  1.2057409700121753\n"
     ]
    }
   ],
   "source": [
    "# entropy (H)\n",
    "\n",
    "def entropy(feature, dataset):\n",
    "    base = 2\n",
    "    en = pd.Series(dataset[feature]).value_counts(normalize=True, sort=False)\n",
    "    ent = -np.sum(en * np.log(en)/np.log(base))\n",
    "\n",
    "    return ent\n",
    "    \n",
    "buying_entropy = entropy('buying', edf)\n",
    "print(\"entropy(buying) =\", buying_entropy)\n",
    "evaluation_entropy = entropy('evaluation', edf)\n",
    "print(\"entropy(evaluation) = \", evaluation_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 3 (10 points)\n",
    "Implement a function that calculates gini index for a feature in a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini(evaluation) = 0.4572837630744171\n"
     ]
    }
   ],
   "source": [
    "# gini index (Gini)\n",
    "\n",
    "def gini(feature, dataset):\n",
    "    \n",
    "    en = pd.Series(dataset[feature]).value_counts(normalize=True, sort=False)\n",
    "    gi = 1-np.sum(en**2)\n",
    "    return gi\n",
    "\n",
    "    \n",
    "evaluation_gini = gini('evaluation', edf) \n",
    "print(\"gini(evaluation) =\", evaluation_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 3 (10 points)\n",
    "Implement a function that calculates information gain (IG) for a feature in a given dataset and the target variable. This function is also expected take a measure of impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain with Entropy: Evaluation\n",
      "buying : 0.09644896916961399\n",
      "maint : 0.09644896916961399\n",
      "doors : 0.09644896916961399\n",
      "persons : 0.09644896916961399\n",
      "lug_boot : 0.09644896916961399\n",
      "safety : 0.09644896916961399\n",
      "\n",
      "Information gain with Gini: Evaluation\n",
      "buying  : 0.014286077889231918\n",
      "maint  : 0.014286077889231918\n",
      "doors  : 0.014286077889231918\n",
      "persons  : 0.014286077889231918\n",
      "lug_boot  : 0.014286077889231918\n",
      "safety  : 0.014286077889231918\n"
     ]
    }
   ],
   "source": [
    "# information gain (IG)\n",
    "\n",
    "def IG(feature, target, dataset, measure):\n",
    "    #calculate impurity for entire dataset\n",
    "    t_impurity = eval(measure)(target,dataset)\n",
    "\n",
    "    e_list = list() #stores entropy og each partition\n",
    "    w_list = list() #stores num.instances in each partition\n",
    "\n",
    "    #interate through each level of the df to partition data set w/ respect to TF \n",
    "    #calculate entropy and weight the level's part. \n",
    "    for level in dataset[feature].unique():\n",
    "        fdf = dataset[dataset[feature] == level]\n",
    "        ent = eval(measure)(target, fdf)\n",
    "        e_list.append(ent)\n",
    "        w_list.append(len(fdf) / len(dataset))\n",
    "        \n",
    "    feature_remaining_impurity = np.sum(np.array(e_list) * np.array(w_list))\n",
    "    information_gain = t_impurity - feature_remaining_impurity\n",
    "    return(information_gain)\n",
    "\n",
    "print(\"Information gain with Entropy: Evaluation\")\n",
    "eval_IG = IG('buying','evaluation', edf, 'entropy')\n",
    "for i in edf.drop(columns='evaluation').columns:\n",
    "    print(i,\":\", eval_IG )\n",
    "\n",
    "print(\"\\nInformation gain with Gini: Evaluation\")\n",
    "eval_gini = IG('buying','evaluation', edf, 'gini')\n",
    "for i in edf.drop(columns='evaluation').columns:\n",
    "    print(i,\" :\", eval_gini )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Question 4 (20 points)\n",
    "Implement the IUFS feature selection as a function that will select k most informative features using information gain based on a target variable. This function will also take a measure of impurity. It will return a list of k feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most informative features:  ['safety', 'persons']\n"
     ]
    }
   ],
   "source": [
    "def IUFS(target, dataset, k, measure='entropy'):\n",
    "    #fill a list with all the entropy scores for each target, sort them, and pick k best features \n",
    "    f_list = []\n",
    "    for f in edf.columns.drop(target):\n",
    "        f_list.append((f, IG(f, target, dataset,measure)))\n",
    "    arr = sorted(f_list, key = lambda x: x[1], reverse=True) [:k]\n",
    "    return [x[0] for x in arr]\n",
    "\n",
    "print(\"Most informative features: \",IUFS('evaluation', edf, 2, measure='entropy') )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Bonus Question (10 points)\n",
    "Improve the IUFS by including an option for gain ratio. Gain ratio is an alternative to information gain and can be used with either of the Gini index or entropy measures. First implement gain ratio (GR), then implement the updated version of IUFS function (IUFS2), which will take a selection metric ('IG' for information gain or 'GR' for gain ratio) as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain ration =  0.019048103852309223\n"
     ]
    }
   ],
   "source": [
    "def GR(feature, target, dataset, measure):\n",
    "    #gr = Ig/measure \n",
    "    gain_ratio = IG(feature, target, dataset, measure) / eval(measure)(feature,dataset)\n",
    "    return gain_ratio\n",
    "\n",
    "print(\"Gain ration = \",GR('buying','evaluation', edf, 'gini')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most informative features with Gain Ratio: \n",
      "['safety', 'persons']\n"
     ]
    }
   ],
   "source": [
    "def IUFS2(target, dataset, k, measure='entropy', gain='IG'):\n",
    "    #fill a list with all the entropy scores for each target, sort them, and pick k best features \n",
    "    f_list = []\n",
    "    for f in edf.columns.drop(target):\n",
    "        f_list.append((f, IG(f, target, dataset,measure)))\n",
    "    arr = sorted(f_list, key = lambda x: x[1], reverse=True) [:k]\n",
    "    return [x[0] for x in arr]\n",
    "    \n",
    "print(\"Most informative features with Gain Ratio: \")\n",
    "print(IUFS2('evaluation', edf, 2, measure='gini', gain='GR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
